 # Fine-Tuning LLaMA 2 7B Model on Domain-Specific IT Data using AWS SageMaker
<h2>ğŸ’»Project Overview</h2>
ğŸŒŸThis project focuses on fine-tuning Meta's LLaMA 2 7B model using AWS SageMaker to adapt it to IT domain-specific knowledge. 
The goal is to transform a pre-trained language model into a domain expert capable of generating accurate, relevant, and context-aware responses for IT-related topics.

The project is submitted as part of the Udacity AI program, showcasing a practical proof of concept (POC) for enhancing business processes through fine-tuned large language models (LLMs). 
This includes documenting the process of finetuning, also compare the outcomes of pretrained and fine-tuned model to represent the performance of domain-expert AI model.



<h2>ğŸ“‚1. AWS Configuration and Setup</h2>

- Installed required libraries:
```bash
  !pip install --upgrade sagemaker datasets
```
- Imported necessary modules like `boto3`, `SageMaker Jumpstart`, and other AWS service libraries.
- Configured AWS SageMaker as the integrated development environment (IDE).
- Created an Amazon S3 bucket to store the domain-specific dataset: `s3://genaiwithawsproject2024/training-datasets/it`

<h2>ğŸ”§2. Deploy and Evaluate Pre-Trained Model</h2>

- Deployed the base Meta LLaMA 2 7B model using AWS SageMaker endpoints.
- Ran inference using IT-specific prompts to evaluate the base model's knowledge and performance.
- Recorded the base model's response as a baseline for later comparison.

<h2>ğŸ“…3. Data Preparation</h2>

- The dataset used for fine-tuning consists of unstructured IT and computer science texts.
- Stored the dataset in the S3 bucket and prepared it for fine-tuning using the SageMaker API.

<h2>ğŸš€4. Model Fine-Tuning</h2>
The fine-tuning process involved adapting the pre-trained LLaMA 2 model to IT-specific language and concepts using transfer learning.

 Fine-tuning involved:

- Leveraged AWS SageMaker training jobs to fine-tune the model on the IT dataset.
- Optimized the hyperparameters to improve domain-specific accuracy while retaining the generalization capabilities of the base model.
- Retaining the model's ability to generate coherent and contextually relevant text.

<h2>ğŸ› ï¸5. Model Deployment and Inference</h2>

- Deployed the fine-tuned model using a new AWS SageMaker endpoint for real-time predictions.
- Conducted inference using the same IT domain-specific prompts used earlier.

<h2>ğŸ”6. Testing and Evaluation</h2>
The fine-tuned model was tested on domain-specific prompts. Its response was compared to those generated by the base model.

 Comparison Criteria:

- Relevance to IT-specific questions.
- Accuracy of responses.
- Contextual understanding and depth.

 <h3> ğŸ¯ Results and Outcomes</h3>
 
- The fine-tuned model demonstrated a significant improvement in understanding and generating IT-specific content.
- Outputs were more accurate, contextually relevant, and tailored to IT knowledge

 <h2>ğŸ¤Acknowledgment</h2>
 This project was submitted as part of the Udacity AI Program, where AWS SageMaker compute resources were generously provided.
I extend my gratitude to Udacity for facilitating the infrastructure required to successfully implement this project.

